{"cells":[{"cell_type":"markdown","metadata":{"id":"ch7HKTAoRXBJ"},"source":["#***Deep Learning Homework 4: Recurrent Neural Networks & Transformer***\n","### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n","### 2nd semester - 6 ECTS\n","### Prof. Alessandro Sperduti, Prof. Nicolò Navarin and Prof. Luca Pasa\n","---\n","In this homework, we will explore how to develop a simple Recurrent Neural Network (RNN) for sentiment analysis. We will use the IMDB dataset---it contains the text of some reviews and the sentiment given by their authors (either positive or negative). The input to the RNN is the sequence of words that compose a review, so the learning task consists in predicting the overall sentiment of the review.\n","In the first part, we will learn how to develop a simple RNN, then we will explore the differences in terms of computational load, number of parameters, and performances with respect to more advanced recurrent models, like LSTM and GRU. Subsequently, we experiment with the bi-directional model to unveil the strengths and the weaknesses of this technique. Finally, we will solve the same classification problem with a Transformer, in order to have a closer look at its internal functioning.\n","\n","---\n","##**Important Instructions for Submissions:**\n","\n","Generally, in the homeworks, you will be either required to complete a part of Python code or to answer questions in text cells. Code and text cells where you are expected to write your answers have been marked by `%STARTCODE` and `%ENDCODE` or `%STARTEXT` and `%ENDTEXT` tags, respectively. Note that you should never change, move or remove these two tags, otherwise your answers will be __not__ valid. As you will see in this notebook, each cell that includes a `[TO COMPLETE]` part has been put between these placeholders."]},{"cell_type":"markdown","metadata":{"id":"fu9LiLxTXt_X"},"source":["#Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"agBmeg2KY_KN"},"outputs":[],"source":["!pip3 install datasets skorch torchinfo torchdata torchtext torchvision matplotlib portalocker>=2.0.0"]},{"cell_type":"markdown","metadata":{"id":"LAaDwXybXvnw"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fU8q5qubXwtf"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import Embedding, RNN, LSTM, GRU, Linear, Transformer\n","import torch.nn.functional as F\n","from torchinfo import summary\n","from torchtext.datasets import IMDB\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchvision.transforms import Lambda\n","from timeit import default_timer as timer\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import gc\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","torch.manual_seed(42)\n","rng = np.random.default_rng(seed=4242)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"sysNS560Xxux"},"source":["#Data Loading and Preprocessing\n","\n","###Load dataset:\n","In this HW, we use the same datset used in the HW2, the IMDB dataset. The dataset contains 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative). As usual, for speed and efficiency, we will use only a subset of the dataset. Reviews have been preprocessed, and each review is encoded as a sequence of word indexes. We load the data from the PyTorch database and then split the data into train, validation and test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQpvhGzaJ87l"},"outputs":[],"source":["train, test = IMDB(root=\"dataset\", split=('train', 'test')) # This is a Torch \"datapipeline\"\n","train.shuffle()\n","test.shuffle()\n","len_train, len_val, len_test = 25000, 12500, 12500 # Whole data\n","# Build validation set\n","valid, test = test.random_split(total_length=len_test*2, weights={\"valid\": 0.5, \"test\": 0.5}, seed=42)\n","# Get data from the pipeline\n","train_data = [x for x in train]\n","test_data = [x for x in test]\n","val_data = [x for x in valid]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6PC19R6Z_AQ"},"outputs":[],"source":["len(train_data), len(val_data), len(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twbK_FAwZ_Cj"},"outputs":[],"source":["idx = 10\n","label_samp, text_samp = train_data[idx]\n","print(f\"text: {text_samp}\")\n","print(f\"label: {label_samp}\")"]},{"cell_type":"markdown","metadata":{"id":"LvhHoF4nT94Q"},"source":["Let's check the dataset statistics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PiJVVkgZ_FJ"},"outputs":[],"source":["datasets = {'train':train_data, 'val':val_data, 'test':test_data}\n","for key in datasets:\n","  label_dist = {}\n","  dataset = datasets[key]\n","  for lb, txt in dataset:\n","    if lb not in label_dist:\n","      label_dist.setdefault(lb, 1)\n","    else:\n","      label_dist[lb] += 1\n","  print(f\"{key}:\")\n","  print(label_dist)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iO3XxTFFZ_J4"},"outputs":[],"source":["tokenizer = get_tokenizer('basic_english')\n","\n","def create_tokens(dataset):\n","  for sample in dataset:\n","    yield tokenizer(sample[1])\n","\n","vocab = build_vocab_from_iterator(create_tokens(train_data), specials=[\"<oov>\", \"<sos>\"], max_tokens=10000)\n","vocab.set_default_index(vocab[\"<oov>\"])\n","print(f\"Our vocabulary is made of {len(vocab)} tokens-index pairs.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"octruo_DZ_MW"},"outputs":[],"source":["text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU-jkLc785iB"},"outputs":[],"source":["print(vocab.get_itos())  # top 10000 freq words (including special chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ohTgQRU6py6"},"outputs":[],"source":["word_index = {k:v for (k, v) in enumerate(vocab.get_itos())}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LgHdz6a6U1g"},"outputs":[],"source":["def decode_review(word_ids):\n","    return \" \".join([word_index.get(word_id, \"<err>\") for word_id in word_ids])\n","\n","print(text_samp)\n","print(\"\\n\")\n","print(decode_review(text_pipeline(text_samp)))"]},{"cell_type":"markdown","metadata":{"id":"RHkfIQpAVkWA"},"source":["To keep the length of the all the input sequences same, we define the padding function. All the sentence less than the lenght of 500 will be padded with zeros and greater than 500 will be truncated. Notice that we will pad and truncate sequences right-wise, so that processing the sequences, the final hidden states of the recurrent networks will correspond to the final words of each sequence in the batch.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ng1AGY-MBvLo"},"outputs":[],"source":["def sent_padding(sent_vec, maxlen):\n","  sent_vec = torch.tensor(sent_vec)\n","  maxlen -= len(sent_vec)\n","  return F.pad(sent_vec, (maxlen, 0))\n","\n","\n","print(sent_padding([1,2,3], maxlen=6))\n","print(sent_padding([1,2,3,4,5,6,7,8,9], maxlen=6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEu2kEw8AitP"},"outputs":[],"source":["seq_len = 500\n","sent_padding(text_pipeline(text_samp), maxlen=seq_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kZBk07MCscA"},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","  def __init__(self, dataset, seq_len=seq_len):\n","    super().__init__()\n","    self.dataset = dataset\n","    self.seq_len = seq_len\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","    label, text = self.dataset[idx]\n","    label = label_pipeline(label)\n","    txt_rep = sent_padding(text_pipeline(text), maxlen=self.seq_len)\n","    label, txt_rep = torch.tensor(label, dtype=torch.float32), torch.tensor(txt_rep, dtype=torch.long)\n","    return label.to(device), txt_rep.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSt_NtvtCse3"},"outputs":[],"source":["train_dataset = CustomDataset(train_data)\n","val_dataset = CustomDataset(val_data)\n","test_dataset = CustomDataset(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LR_4LyNCshb"},"outputs":[],"source":["batch_size = 256\n","dataloader_training = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","dataloader_validation = DataLoader(val_dataset, batch_size=batch_size)\n","dataloader_test = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"Jsd_sk1pX04P"},"source":["# Model Definition [TO COMPLETE]\n","\n","#Model Definition\n","\n","Let's define the model:\n","- The first layer is an Embedding layer, with input_dim=vocab_dim and output_dim=10. The model will gradually learn to represent each of the 10,000 words as a 10-dimensional vector. So the next layer will receive 3D batches of shape (batch size, 500, 10)\n","- The second layer is the recurrent one. In particular, in this case, we use a [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), LSTM and GRU.\n","- The output layer is linear one that maps the model output to the target space and applies a sigmoid function (we are using a [Binary Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss) which, differently from Cross-Entropy Loss, expects probabilities as input)\n","\n","### Model Comparison\n","\n","In order to perform a fair comparison of the models RNN, LSTM and GRU make sure they use more or less the same number of parameters.\n","\n","In the next cell, we define our simple RNN used for binary classification. The class has two main methods, the constructor (`init()`) and the `forward()` method.\n","\n","In the constructor, the input parameters are used to define the layers and hyperparameters of the RNN. The layers that are defined include an embedding layer (`self.embedding`), a recurrent layer (`self.rnn`), and a linear layer (`self.linear`). The constructor also sets up various parameters such as the embedding input size, the embedding output size, the hidden size, the number of layers, the batch size, the RNN type, and whether or not the RNN is bidirectional.\n","\n","The `forward()` method takes a batch of input data (`x`) and applies the layers defined in the constructor in a specific sequence. First, the input is passed through the embedding layer to create embeddings of the input tokens. These embeddings are then permuted to be of the correct shape for the RNN layer, which expects inputs of the form (`seq_len, batch_size, H_in`). The RNN layer is then applied to these embeddings, producing both the RNN output (`rnn_out`) and the last hidden state (`self.last_hidden`). Finally, the output of the RNN is passed through a linear layer and flattened to produce the final output of the network, which is a sigmoid activation function applied to a tensor of shape (`batch_size`). This output is then returned.\n","\n","**[TO COMPLETE]** Implement the forward method."]},{"cell_type":"markdown","metadata":{"id":"oPb_W9K5Zyk-"},"source":["`%STARTCODE%`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTY5n-t7X3VD"},"outputs":[],"source":["class My_RNN(nn.Module):\n","  def __init__(self, vocab_length, emb_dim, hidden_size, num_layers, rnn_dropout,\n","               batch_size, RNN_type, bidirectional, device=device):\n","    super().__init__()\n","    self.emb_in_dim = vocab_length  # 10000\n","    self.emb_out_dim = emb_dim  # 10\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.bidirectional = bidirectional\n","    self.batch_size = batch_size\n","    self.RNN_type = RNN_type\n","    self.target_size = 1  # binary classification\n","    self.device = device\n","    self.D = 2 if self.bidirectional else 1  # enable/disable bidirectional\n","\n","    valid_types = {'Simple RNN':RNN, 'LSTM':LSTM, 'GRU':GRU}\n","    assert self.RNN_type in valid_types.keys(), f'You must choose one of {valid_types.keys()} types'\n","\n","    self.embedding = Embedding(self.emb_in_dim, self.emb_out_dim)\n","\n","    chosen_rnn = valid_types[self.RNN_type]\n","    self.rnn = chosen_rnn(input_size=self.emb_out_dim, hidden_size=self.hidden_size,\n","                          num_layers=self.num_layers, dropout=rnn_dropout, bidirectional = self.bidirectional)\n","\n","    self.linear = Linear(self.D*self.hidden_size, self.target_size)\n","\n","  def forward(self, x):\n","    # x: (batch_size, 500)\n","    # [TO COMPLETE]\n","\n","    # Embeds have shape (batch_size, 500, 10)\n","    # But rnn receives inputs of: (500, batch_size, 10) (seq_len, batch_size, H_in)\n","    # [TO COMPLETE]\n","\n","    rnn_out, self.last_hidden =  # [TO COMPLETE]\n","    # rnn_out: (500, 256, 2*5:10) -> (seq_len, batch_size, D*hidden_size)\n","    # h_n: (1, 256, hidden_size) -> (D*num_layers, batch_size, hidden_size)\n","\n","    # hint: pay attention to WHICH output dimension corresponds to the elements of the sequence\n","    # [TO COMPLETE]\n","\n","    #output: (256, 1) -> (batch_size, target_size)\n","    output = output.flatten()\n","    #output: (256) -> (batch_size)\n","    return F.sigmoid(output)"]},{"cell_type":"markdown","metadata":{"id":"L_Iv4QVcZyk_"},"source":["`%ENDCODE%`"]},{"cell_type":"markdown","metadata":{"id":"QmakxXHJjlJR"},"source":["\n","The code below allows us to do a verifcation of the model in term of any implementation error. The script demonstrates the process of initializing a recurrent neural network model, loading data, performing inference, and inspecting the output using a simple example.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5d2_4VlPH1i"},"outputs":[],"source":["'''\n","CHECK I\n","'''\n","vocab_length = len(vocab)\n","emb_dim = 10\n","hidden_size = 5\n","num_layers = 2\n","RNN_type = 'LSTM' #possible choices -> ['Simple RNN', 'LSTM', 'GRU']\n","bidirectional = True\n","#BiDirectional GRU with 2 layers of hidden size of 5\n","\n","model = My_RNN(vocab_length=vocab_length, emb_dim=emb_dim, hidden_size=hidden_size,\n","               num_layers=num_layers, batch_size=batch_size, rnn_dropout=0, RNN_type=RNN_type,\n","               bidirectional=bidirectional).to(device)\n","\n","label, text = next(iter(dataloader_training))\n","x = model.forward(text)\n","print(x.shape)\n","print(x[52]) #sigmoid output"]},{"cell_type":"markdown","metadata":{"id":"vdKi6_PtkI_A"},"source":["Here, we use the My_RNN class to check the model summary for different models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmPnOvlYTIdd"},"outputs":[],"source":["vocab_length = len(vocab)\n","emb_dim = 10\n","hidden_size = 32\n","num_layers = 1\n","RNN_type = 'Simple RNN' #possible choices -> ['Simple RNN', 'LSTM', 'GRU']\n","bidirectional = False\n","lr = 1e-3\n","\n","model = My_RNN(vocab_length=vocab_length, emb_dim=emb_dim, hidden_size=hidden_size,\n","               num_layers=num_layers, batch_size=batch_size, rnn_dropout=0, RNN_type=RNN_type,\n","               bidirectional=bidirectional).to(device)\n","\n","summary(model)\n","#if you see there is a differnce of 1408-1376=32 with the keras version\n","#that is because of the fact that keras does not consider bias terms for RNNs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmkiEic1ToNL"},"outputs":[],"source":["criterion = torch.nn.BCELoss()  # does not apply sigmoid\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1I8ZE7PKf8mS"},"outputs":[],"source":["'''\n","Sanity Check II: Overfitting on one batch\n","'''\n","E = 500\n","label, text = next(iter(dataloader_training))\n","for itr in range(E):\n","    model.train()\n","    optimizer.zero_grad()\n","    logits = model(text)\n","    loss = criterion(logits, label)\n","    if itr % 100 == 0 or itr == E-1:\n","      print(f\"epoch: {itr} -> Loss: {loss}\")\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"9Vgp7U3QNBsG"},"source":["# Simple RNN\n","\n","We define a Simple RNN model and evaluate its performance. We define the training and evaluation loop. The function takes the model, optimizer, dataloader_train, dataloader_val and epochs as input parameters. The function returns training loss, validation loss, training accuracy and validation accuracy to monitor the model performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPy8m4hPUGbs"},"outputs":[],"source":["def train(model, optimizer, dataloader_train, dataloader_val, epochs):\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(epochs):\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (label, text) in enumerate(dataloader_train):\n","      optimizer.zero_grad()\n","      logits = model(text)\n","      loss = criterion(logits, label)\n","      total_loss_train += loss\n","      loss.backward()\n","      optimizer.step()\n","\n","      labels_form_logits = lambda x: 0. if x < 0.5 else 1.\n","      logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n","      total_acc_train += (logits == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","\n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (label, text) in enumerate(dataloader_val):\n","            logits = model(text)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n","            total_acc_val += (logits == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val)\n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","  return loss_train, acc_train, loss_val, acc_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uX9Zb5OPVuFV"},"outputs":[],"source":["def plot_learning_acc_and_loss(loss_tr, acc_tr, loss_val, acc_val):\n","\n","    plt.figure(figsize=(8, 10))\n","\n","    plt.subplot(2, 1, 1)\n","    plt.grid()\n","    plt.plot(range(EPOCHS), acc_tr, label='acc_training')\n","    plt.plot(range(EPOCHS), acc_val, label='acc_validation')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='best')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.grid()\n","    plt.plot(range(EPOCHS), loss_tr, label='loss_training')\n","    plt.plot(range(EPOCHS), loss_val, label='loss_validation')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend(loc='best')\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wQp313iXZylB"},"source":["## Train an RNN [TO COMPLETE]\n","\n","Your task is then to train the Deep Learning model:\n","* keep the number of epochs fixed (same for all models)\n","* do not overfit the training set: we care about generalization, so performance on the val set should be reasonably close (withing a few % pts) to that of the training set. You can use regularization techniques such as dropout and weight decay\n","* try to get a good performance (at least 70% validation accuracy!)\n","* keep in the cells below only the outputs of the best model configuration you find."]},{"cell_type":"markdown","metadata":{"id":"bFWRRc8IZylB"},"source":["`%STARTCODE%`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCxnJfEAi6aX"},"outputs":[],"source":["emb_dim =   # [TO COMPLETE]\n","hidden_size =   # [TO COMPLETE]\n","num_layers =   # [TO COMPLETE]\n","rnn_dropout =   # [TO COMPLETE]\n","lr =   # [TO COMPLETE]\n","\n","vocab_length = len(vocab)\n","EPOCHS = 75\n","\n","model = My_RNN(vocab_length=vocab_length, emb_dim=emb_dim, hidden_size=hidden_size,\n","               num_layers=num_layers, batch_size=batch_size, rnn_dropout=rnn_dropout, RNN_type='Simple RNN',\n","               bidirectional=False).to(device)\n","\n","criterion = torch.nn.BCELoss() #does not apply sigmoid\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAtPJWvSZylB"},"outputs":[],"source":["summary(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSiNr8G0UQUK"},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, epochs=EPOCHS)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"873TADzanhB9"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vaVsXbkXjQ1"},"outputs":[],"source":["def test(model, dataloader_test=dataloader_test):\n","  model.eval()\n","  total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n","  for idx, (label, text) in enumerate(dataloader_test):\n","      pre_label = model(text)\n","      loss += criterion(pre_label, label)\n","      labels_form_pre_label = lambda x: 0. if x < 0.5 else 1.\n","      pre_label = torch.tensor(list(map(labels_form_pre_label, pre_label))).to(model.device)\n","      total_acc_test += (pre_label == label).sum().item()\n","      total_count_test += label.size(0)\n","      n_batches_test += 1\n","  accuracy_test = total_acc_test/total_count_test\n","  loss_test = loss/n_batches_test\n","  print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n","  print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C3jbfYYWXjeM"},"outputs":[],"source":["test(model)"]},{"cell_type":"markdown","metadata":{"id":"Rj8Jn9CbZylB"},"source":["`%ENDCODE%`"]},{"cell_type":"markdown","metadata":{"id":"8CkfgH3KZylC"},"source":["## Improve over the baseline [TO COMPLETE]\n","* Try to improve over the result you obtained with the RNN using a different architecture. You can experiment with GRU, LSTM and BiLSTM\n","* Do not change the number of epochs\n","* Try to keep the number of parameters the same as that of the RNN, so that you can make a fair comparison between different architectures\n","* Do not overfit the training set: as before, the performance of the validation set should be within a few percentage points that on the training set. You can use regularization techniques such as dropout and weight decay\n","* You should be able to reach at least 80% accuracy on the validation set\n","* Report in a table a comparison of a few models and hyperparameters configurations that you tried. Report: model name (LSTM, GRU, etc), hyperparameters values (emb_dim, hidden_size, num_layers, lr, dropout, etc), number of parameters, training accuracy, validation accuracy, training time.\n","* Keep in the cells below only the outputs of the best training run\n","* at the end, discuss and report your results."]},{"cell_type":"markdown","metadata":{"id":"ErvXfLKlZylC"},"source":["`%STARTCODE`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHvTKfDkLiti"},"outputs":[],"source":["vocab_length = len(vocab)\n","emb_dim = #[TO COMPLETE]\n","hidden_size = #[TO COMPLETE]\n","num_layers = #[TO COMPLETE]\n","RNN_type = #[TO COMPLETE] # possible choices -> ['Simple RNN', 'LSTM', 'GRU']\n","rnn_dropout = #[TO COMPLETE]\n","lr = #[TO COMPLETE]\n","bidirectional = False\n","\n","EPOCHS = 75\n","\n","\n","model = My_RNN(vocab_length=vocab_length, emb_dim=emb_dim, hidden_size=hidden_size,\n","               num_layers=num_layers, batch_size=batch_size, rnn_dropout=rnn_dropout, RNN_type=RNN_type,\n","               bidirectional=bidirectional).to(device)\n","\n","criterion = torch.nn.BCELoss()#does not apply sigmoid\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","summary(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2lJqnakLiwD","scrolled":true},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, epochs=EPOCHS)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rf4xauBAR5Gu"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHX360zUN2ET"},"outputs":[],"source":["test(model)"]},{"cell_type":"markdown","metadata":{"id":"pdQfMq4QpcFp"},"source":["**[TO COMPLETE]** Discuss and report your results:"]},{"cell_type":"markdown","metadata":{"id":"Iu3AMCyeZylD"},"source":["|Model name|Hyperparameters cfg|# parameters|Accuracy (Tr)|Accuracy (Val)|Training time|\n","|-|-|-|-|-|-|\n","|..|..|..|..|..|..|"]},{"cell_type":"markdown","metadata":{"id":"hiCQwYGuZylD"},"source":["`%ENDCODE%`"]},{"cell_type":"markdown","metadata":{"id":"5kk32TU_ajUS","jp-MarkdownHeadingCollapsed":true},"source":["# Explore relations between words"]},{"cell_type":"markdown","metadata":{"id":"fjv9g8A0Fs3X"},"source":["We will now quickly explore the properties of the embeddings learned by the model. Each embedding encodes the meaning of a word inferring it from the way it is used in the dataset. One possible way to explore the meaning encoded in the embeddings is check whether analogies that we make between concepts are reflected also in the embeddings as geometric properties. In particular, we will compute the difference between the embeddings of two related words, thus encoding their relation in a vector. Then, we will compute the same measure between a few couples of vectors and if the couple which has the most similar measure corresponds to words that have the same kind of relation as the first ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5ljHjPJak_D"},"outputs":[],"source":["def word_embedder(word):\n","  word_high_dim = sent_padding(text_pipeline(word), maxlen=seq_len).to(device)\n","  word_low_dim_embedded = model.embedding(word_high_dim)[-1]\n","  return word_low_dim_embedded\n","\n","EMB_VOCAB = {}\n","for word in vocab.get_itos():\n","  EMB_VOCAB[word] =  word_embedder(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t64LwUq7iO6F"},"outputs":[],"source":["def cosine_similarity(u, v):\n","    return torch.dot(u,v) / (torch.sqrt(torch.sum(u * u)) * torch.sqrt(torch.sum(v * v)))\n","\n","def word_matcher(ref1, ref2, tgt3, candidates, EMB_VOCAB):\n","  max_thr = -np.inf\n","  e_ref1, e_ref2, e_tgt3 = EMB_VOCAB[ref1], EMB_VOCAB[ref2], EMB_VOCAB[tgt3]\n","  for w in candidates:\n","    e_w = EMB_VOCAB[w]\n","    sim = cosine_similarity(e_ref2 - e_ref1, e_w - e_tgt3)\n","    if sim > max_thr:\n","      result = w\n","      max_thr = sim\n","  return result\n","\n","#w1, w2, w3 = 'man', 'woman', 'doctor'\n","#w4_cand = ['doctor', 'dentist', 'nurse', 'illness']\n","#w4 = word_matcher(w1, w2, w3, w4_cand, EMB_VOCAB)\n","#print(f'The relation between {w1} -> {w2} is like the realtion between {w3} -> {w4}')\n","\n","w1, w2, w3 = 'man', 'woman', 'king'\n","w4_cand = ['soldier', 'queen', 'prophet']\n","w4 = word_matcher(w1, w2, w3, w4_cand, EMB_VOCAB)\n","print(f'The relation between {w1} -> {w2} is like the realtion between {w3} -> {w4}')\n","\n","w1, w2, w3 = 'germany', 'berlin', 'italy'\n","w4_cand = ['spain', 'rome', 'germany', 'france']\n","w4 = word_matcher(w1, w2, w3, w4_cand, EMB_VOCAB)\n","print(f'The relation between {w1} -> {w2} is like the realtion between {w3} -> {w4}')"]},{"cell_type":"markdown","metadata":{"id":"nBzxjzuvaTiC"},"source":["# Transformer [TO COMPLETE]"]},{"cell_type":"markdown","metadata":{"id":"9-IuFF0KGL0E"},"source":["Let's now use a [Transformer](https://arxiv.org/abs/1706.03762) to perform the same task considered in the previous exercise.\n","\n","The structure of the transformer is defined as follows:\n","*   A multi-head attention layer\n","*   Dropout operation (`dropout_att`)\n","*   Layer Normalization (`layernorm_att`)\n","*   A feedforward Neural Network, Sequential, and Dense layer\n","*   Dropout operation (`dropout_fnn`)\n","*   Layer Normalization (`layernorm_fnn`) that has in input the summation of the attention layer output and the feedforward NN output\n","\n","Your task is to experiment with different hyperparameters values and try to find a configuration of the Transformer that can beat the RNNs. In the cell below, write a short comment on the impact of each hyperparameter on model performance according to your observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvDpePVEbqkF"},"outputs":[],"source":["new_seq_len = 200\n","train_dataset_trns = CustomDataset(train_data, seq_len=new_seq_len)\n","val_dataset_trns = CustomDataset(val_data, seq_len=new_seq_len)\n","test_dataset_trns = CustomDataset(test_data, seq_len=new_seq_len)\n","\n","batch_size=256\n","dataloader_training_trns = DataLoader(train_dataset_trns, batch_size=batch_size, shuffle=True)\n","dataloader_validation_trns = DataLoader(val_dataset_trns, batch_size=batch_size)\n","dataloader_test_trns = DataLoader(test_dataset_trns, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHI6PHorfKmo"},"outputs":[],"source":["class My_Transform(nn.Module):\n","  def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, layer_norm_eps, device=device):\n","    super().__init__()\n","    self.device = device\n","\n","    self.emb_en = Embedding(num_embeddings=len(vocab), embedding_dim=d_model)\n","    self.emb_de = Embedding(num_embeddings=2, embedding_dim=d_model)\n","    #2: because of binary classification\n","\n","    #d_model (int) – the number of expected features in the encoder/decoder inputs (default=512).\n","    #nhead (int) – the number of heads in the multiheadattention models (default=8).\n","    #num_encoder_layers (int) – the number of sub-encoder-layers in the encoder (default=6).\n","    #num_decoder_layers (int) – the number of sub-decoder-layers in the decoder (default=6).\n","    #dim_feedforward (int) – the dimension of the feedforward network model (default=2048).\n","    #dropout (float) – the dropout value (default=0.1).\n","    #layer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).\n","    self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n","                                   num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n","                                   dropout=dropout, layer_norm_eps=layer_norm_eps)\n","\n","    self.linear = Linear(d_model, 1)#1: because of binary classification\n","\n","  def forward(self, x, y):\n","\n","    #x: (batch_size, seq_len) -> (256, 200)\n","    #y: (batch_size) -> (256)\n","\n","    y = torch.roll(y, shifts=1, dims=0) #right shifted\n","\n","    self.embedded_src = self.emb_en(x)\n","    #self.embedded_src: (batch_size, seq_len, d_model) -> (256, 200, 5)\n","\n","    '''\n","    Transformer requires src_dim and trg_dim of (S, N, E)\n","    (S, N, E) -> (Seq_len, Batch_size, Embed_dim) -> (seq_len, batch_size, d_model)\n","        - self.embedded_src must be permuted\n","        - self.embedded_trg must be reshaped accordingly\n","    Transformer will produce an output of dim (T, N, E)\n","    (T, N, E) -> (Target_len, Batch_size, Embed_dim) -> (1, batch_size, d_model)\n","    '''\n","\n","    self.embedded_src = self.embedded_src.permute(1, 0, 2)\n","    #self.embedded_src: (seq_len, batch_size, d_model) -> (200, 256, 5)\n","    self.embedded_trg = self.emb_de(y.unsqueeze(0).long())\n","    #self.embedded_trg: (seq_len:target_len, batch_size, d_model) -> (1, 256, 5)\n","\n","    trns_out = self.transformer(self.embedded_src, self.embedded_trg)\n","    #trns_out: (target_len, batch_size, d_model) -> (1, 256, 5)\n","\n","    out = self.linear(trns_out)\n","    #out: (1, 256, 1)\n","\n","    return F.sigmoid(out.squeeze()) #(batch_size) -> (256) : Like y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2GQaxwUfKpG"},"outputs":[],"source":["'''\n","Transformer Check\n","'''\n","lb, txt = next(iter(dataloader_training_trns))\n","trns = My_Transform(5, 1, 1, 1, 2, 1e-5, 0.2).to(device)\n","#####################\n","gc.collect()\n","#####################\n","out = trns.forward(txt, lb)\n","assert out.shape == lb.shape, \"Ops, something is wrong!\"\n","print(out.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-8Bii1lbBNO"},"outputs":[],"source":["def train_trns(model, optimizer, dataloader_train, dataloader_val, epochs=EPOCHS):\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(epochs):\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (label, text) in enumerate(dataloader_train):\n","\n","      optimizer.zero_grad()\n","      logits = model(text, label)\n","      loss = criterion(logits, label)\n","      total_loss_train += loss\n","      loss.backward()\n","      optimizer.step()\n","\n","      labels_form_logits = lambda x: 0. if x < 0.5 else 1.\n","      logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n","      total_acc_train += (logits == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","\n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (label, text) in enumerate(dataloader_val):\n","\n","            logits = model(text, label)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n","            total_acc_val += (logits == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val)\n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","  return loss_train, acc_train, loss_val, acc_val"]},{"cell_type":"markdown","metadata":{"id":"cypH_4lQZylF"},"source":["Now you can try to further improve performance using a transformer.\n","* as before, do not overfit the training set and do not change the number of epochs\n","* you should be able to reach around 85% validation accuracy\n","* keep in the cells below only the configuration and outputs of the best run"]},{"cell_type":"markdown","metadata":{"id":"3PW1uh9cZylF"},"source":["`%STARTCODE%`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eg7oohRFUUqm"},"outputs":[],"source":["d_model=  #[TO COMPLETE]\n","nhead=  #[TO COMPLETE]\n","num_encoder_layers=  #[TO COMPLETE]\n","num_decoder_layers=  #[TO COMPLETE]\n","dim_feedforward= #[TO COMPLETE]\n","dropout= #[TO COMPLETE]\n","lr =  #[TO COMPLETE]\n","\n","\n","layer_norm_eps=1e-5\n","EPOCHS = 75\n","\n","transformer = My_Transform(d_model, nhead, num_encoder_layers, num_decoder_layers,\n","                           dim_feedforward, dropout, layer_norm_eps).to(device)\n","\n","criterion = torch.nn.BCELoss() #does not apply sigmoid\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=lr)\n","\n","summary(transformer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7oNBz4SaAQQ","scrolled":true},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train_trns(transformer, optimizer, dataloader_training_trns, dataloader_validation_trns, epochs=EPOCHS)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_4Z_R15dc5L"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3FH0yRvb2o3"},"outputs":[],"source":["@torch.no_grad()\n","def test_trns(model, dataloader_test):\n","  model.eval()\n","  total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n","  for idx, (label, text) in enumerate(dataloader_test):\n","      pre_label = model(text, label)\n","      loss += criterion(pre_label, label)\n","      labels_form_pre_label = lambda x: 0. if x < 0.5 else 1.\n","      pre_label = torch.tensor(list(map(labels_form_pre_label, pre_label))).to(model.device)\n","      total_acc_test += (pre_label == label).sum().item()\n","      total_count_test += label.size(0)\n","      n_batches_test += 1\n","  accuracy_test = total_acc_test/total_count_test\n","  loss_test = loss/n_batches_test\n","  print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n","  print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrNFjrQKc0iY"},"outputs":[],"source":["test_trns(transformer, dataloader_test_trns)"]},{"cell_type":"markdown","metadata":{"id":"e1U9hCPnZylG"},"source":["`%ENDCODE%`"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1OZ3CIhUvmPyeZV663MPxItvHgjbrgpyC","timestamp":1715789249054}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}