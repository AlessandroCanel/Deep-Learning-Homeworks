{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Deep Learning Homework 1: *from the Perceptron to Deep Neural Network***\n",
    "### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n",
    "### 2nd semester - 6 ECTS\n",
    "### Prof. Alessandro Sperduti, Prof. Nicol√≤ Navarin and Dr. Luca Pasa\n",
    "---"
   ],
   "metadata": {
    "id": "SY5WztYNneGg"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8MTQQX3WaFa"
   },
   "source": [
    "In this first homework, we are going to write our own simple feedforward neural network using `Python` and `NumPy` (the standard numeric library for Python). We will start by implementing just a simple neuron, or perceptron, then we define the training algorithm for this simple model.\n",
    "The second part consists in defining a simple neural network to perform classification and regression in real-world cases."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Important Instructions for Submissions**\n",
    "\n",
    "Generally, in the homeworks, you will be either required to complete a part of Python code or to answer questions in text cells. Code and text cells where you are expected to write your answers have been marked by `%STARTCODE` and `%ENDCODE` or `%STARTEXT` and `%ENDTEXT` tags, respectively. Note that you should never change, move or remove these two tags, otherwise your answers will be __not__ valid. As you will see in this notebook, each cell that includes a `[TO COMPLETE]` part has been put between these placeholders.\n",
    "\n",
    "As an example, if the task is to _\"define a variable named `x` and assign it to number 2\"_, the following answer style is presented:"
   ],
   "metadata": {
    "id": "EnhjgoafUY69"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTCODE`"
   ],
   "metadata": {
    "id": "1F8KAFuXVp_Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = 2 # [TO COMPLETE]"
   ],
   "metadata": {
    "id": "UrBK7IsgUozD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDCODE`"
   ],
   "metadata": {
    "id": "DU5RrnqiVsk9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, if the task is a question-answering one, the same style is used. As an instance, the proper answer to the question \"How can we import the numpy library in python?\", must seem like the following:"
   ],
   "metadata": {
    "id": "GcAjL4TOWEbn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTEXT`"
   ],
   "metadata": {
    "id": "dvKdI4PBWf4W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Answer: **[TO COMLPETE]**\n",
    "\n",
    "In order to do so, we use the keyword `import`, accompanied by the name of the library we would like to add to our notebook."
   ],
   "metadata": {
    "id": "BylXZkvyWk1c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDTEXT`"
   ],
   "metadata": {
    "id": "uA74CJWmWhtM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As already mentioned, all `%START` and `%END` keywords have already been placed and you just need to be careful not to delete, move or change them. Now let's start with the first homework!"
   ],
   "metadata": {
    "id": "fmaJhiPFXaXv"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bEFm73cYFQy"
   },
   "source": [
    "## Exercise 1.1: Perceptron\n",
    "\n",
    "In this first exercise, we will implement a simple neuron, or perceptron, as described below. We will have just three inputs and one output neuron (we omit the bias term for now).\n",
    "Notice how the perceptron simply performs a sum of the individual inputs multiplied by the corresponding weights mapped through an activation function $\\sigma(\\cdot)$.  This can also be expressed as a dot product of the weight vector $\\textbf{W}$ and the input vector $\\textbf{x}$, thus: $$\\hat{y}=\\sigma(\\textbf{W}^T \\textbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDlidWmiYuKB"
   },
   "source": [
    "We will begin by implementing the perceptron by using the [numpy](https://docs.scipy.org/doc/numpy/reference/) library:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i0UCur_TYckH"
   },
   "source": [
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQONq1k6Y1Zx"
   },
   "source": [
    "### Training data\n",
    "\n",
    "Let's consider a very simple dataset. The dataset is made of four input vectors $\\textbf{x} \\in \\mathbb{R}^3$ and the corresponding desired target values $y$. In the table below, each row is a single sample; the first three columns are the input vector components, whereas the last column is the target output.\n",
    "\n",
    "||Input $x_i$||Output $y$|\n",
    "|:----:|:---:|:---:|---:|\n",
    "| 1    | 1   | 0   | 1  |\n",
    "| 1    | 0   | 0   | 1  |\n",
    "| 0    | 1   | 0   | 0  |\n",
    "| 0    | 0   | 0   | 0  |\n",
    "\n",
    "Notice that our target outputs are equal to the first component of the input, therefore the task that the model should learn is very simple. We will see how the perceptron is able to learn that starting from this toy dataset.\n",
    "\n",
    "Now let's define the `X` and `y` matrices:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u1FzAUxhY9PA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af027370-d801-4a15-b74b-4232707abe73"
   },
   "source": [
    "# Our input data is a matrix, each row is one input sample\n",
    "X = np.array([[1,1,0],\n",
    "              [1,0,0],\n",
    "              [0,1,0],\n",
    "              [0,0,0]])\n",
    "\n",
    "# The target output is a column vector in 2-D array format (.T means transpose)\n",
    "y = np.array([[1,1,0,0]]).T\n",
    "\n",
    "print('X =', X)\n",
    "print('y =', y)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-W8wMnoZH71"
   },
   "source": [
    "### Activation function\n",
    "\n",
    "As we said before, in order to define a perceptron we need to define the activation function $f(\\cdot)$. There are many possibile activation function that can be used, let's plot some of the most common ones:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fi5ZK9zKZssu",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "outputId": "d6ec6fa9-2b83-436b-fc2c-595ba7c07c0a"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = np.arange(-4,4,.01)\n",
    "plt.figure()\n",
    "plt.plot(x_axis, np.maximum(x_axis,0), label='ReLu')\n",
    "plt.plot(x_axis, 1/(1+np.exp(-x_axis)), label='Sigmoid')\n",
    "plt.plot(x_axis, np.tanh(x_axis), label='Tanh')\n",
    "plt.axis([-4, 4, -1.1, 1.1])\n",
    "plt.title('Some Activation Functions')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdmJ6-qaZjuZ"
   },
   "source": [
    "In this particular exercise we will use the sigmoid function. So let's define $f(\\cdot)$ as the sigmoid function\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+\\exp^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wwnk5RgKZRox"
   },
   "source": [
    "def sigma(x):\n",
    "    # Sigmoid function\n",
    "    return 1 / ( 1 + np.exp(-x) )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX-62n3Io0XU"
   },
   "source": [
    "### Weight initialization\n",
    "\n",
    "Now we have to initialise the weights. Let's initialize them randomly, so that their mean is zero. The weights matrix maps the input space into the output space, therefore in our case $\\mathbf{W} \\in \\mathbb{R}^{3 \\times 1}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9f83YkaNtd5h",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4627d7d3-083f-4e6d-ec93-9e347e15ff2a"
   },
   "source": [
    "# fix random seed generator for reproducibility\n",
    "rng = np.random.default_rng(seed=[42,1])\n",
    "\n",
    "# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n",
    "W = 2 * rng.random(size=(3,1)) - 1\n",
    "\n",
    "print('W =', W)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZd-aQxAuX49"
   },
   "source": [
    "### Forward propagation\n",
    "\n",
    "Next, let's try to implement one round of forward propagation.  This means taking an input sample and moving it forward through the network, calculating the output of the network eventually.\n",
    "\n",
    "For our single neuron this is simply $\\hat{\\mathbf{y}} = \\sigma(\\mathbf{W}^T \\mathbf{x})$, where $\\mathbf{x}$ is one input vector.\n",
    "\n",
    "Each input sample is arranged as a row of the matrix `X`, therefore we can access the first row by `X[0]`. Let's store it in the variable `X0` for easier access. We'll use `reshape` to make sure it's expressed as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9gTx4QxUumvQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "47495f2d-1781-43f9-9114-d4ead03c3c25"
   },
   "source": [
    "X0 = np.reshape(X[0], (3,1))\n",
    "print(\"X0 =\", X0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpDCnmxWurjC"
   },
   "source": [
    "The output $\\hat{y}$ for the first input can be calculated according to the formula given above"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "idDZfTVKuyPf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c3d0ffcc-f906-427d-f765-018f1194bfe6"
   },
   "source": [
    "y_out = sigma(np.dot(W.T, X0))\n",
    "\n",
    "print('y_out =', y_out)\n",
    "print('y[0] =', y[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oda3Y3btu8Uz"
   },
   "source": [
    "the target result is stored in `y[0]`.  If you check back, you can see we defined it to be $y_0=1$. You can see that our network is sill far away from the right answer... this is why we need to backpropagate the error, to adjust the weights in the right direction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGvgZCDFu-EM"
   },
   "source": [
    "### Backpropagation\n",
    "\n",
    "The following step is updating the weights by propagating the error backwards in the network.  How this is done depends on the activation function, and namely on its derivative. The activation function of the considered model is the sigmoid, and its derivative is:\n",
    "\n",
    "$$\\sigma(x)'=\\sigma(x) \\cdot (1-\\sigma(x))$$\n",
    "\n",
    "Recall that the weight update in general is given as $\\Delta w_{ji} = -\\epsilon \\delta_j x_i$.\n",
    "Our network has only one layer, so $x_i$ is just the input $\\mathbf{x}$, and a single output neuron so there is no actual need for index $j$.\n",
    "\n",
    "In matrix form we can calculate this for all the weights:\n",
    "\n",
    "$$\\Delta \\textbf{W} = -\\epsilon \\delta \\textbf{x}_0$$\n",
    "where $\\delta$ is the gradient (called `grad` in the following code; see the lecture material for its derivation), $œµ$ is the learning rate, and $\\textbf{x}_0$ is our first input sample in variable `X0`.\n",
    "\n",
    "Recall that $y$ is the desired output, i.e. `y[0]` in this Python code, and $\\hat{y}$ is our predicted value called `y_out` here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bs0EG3n9ve6J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c4e1d766-fae6-49cf-cc76-3e37425ef4cc"
   },
   "source": [
    "# the learning rate determines the step size in the gradient descent, you can experiment with different values if you want\n",
    "learning_rate = 0.5\n",
    "\n",
    "# compute the gradient term\n",
    "grad = (y_out - y[0]) * y_out * (1 - y_out)\n",
    "\n",
    "# Calculate the weight update\n",
    "W_delta = - learning_rate * grad * X0\n",
    "\n",
    "print(\"W_delta = \", W_delta)\n",
    "\n",
    "# Update the weights\n",
    "W += W_delta\n",
    "print(\"Updated weights W = \", W)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha1gJ0n9v2oV"
   },
   "source": [
    "Let's try a forward propagation again with the same input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fh40oHl2v6rs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "39853c8d-d35d-479d-f274-473d81c572fb"
   },
   "source": [
    "print(\"y_out_old = \", y_out) # let's print the values before the update\n",
    "y_out = sigma(np.dot(W.T, X0))\n",
    "\n",
    "print('y_out =', y_out)\n",
    "print('y[0] =', y[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHc3os_AL54y"
   },
   "source": [
    "You should notice that the result has moved (slightly!) towards the correct answer. In order to converge to the right value we have to perform more iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8INLy50fyORN"
   },
   "source": [
    "### Q1: Training iterations **[TO COMPLETE]**\n",
    "\n",
    "Let's define a complete training procedure for our model. In each iteration we have to perform the forward propagation, then we'll check how much the output differs from the target and propagate the error back (backward propagation).  We'll do this for each sample data point and then iterate this over and over again using a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTCODE`"
   ],
   "metadata": {
    "id": "pide_kEzSjEP"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PPXJwQm4ycgH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "afe34d07-21a6-4371-b9b4-02a2152e161f"
   },
   "source": [
    "# For the training we need to iterate over the dataset several times\n",
    "num_iters = 1000\n",
    "\n",
    "# We'll also store the mean square error (MSE) in every round so we can see how it evolves\n",
    "# mse is just an array to store these values at each round:\n",
    "mse = np.zeros(num_iters)\n",
    "\n",
    "# Looping for the iterations\n",
    "for it in range(num_iters):\n",
    "\n",
    "    # For-loop going over each sample in X\n",
    "    for n in range(len(X)):\n",
    "        # Extract the n_th sample and the corresponding desired output\n",
    "        x_n = np.reshape(X[n], (3,1))\n",
    "        # Get the correponding target value\n",
    "        y_target = y[n].item() # item() is needed to treat 1-dim arrays as scalar\n",
    "\n",
    "        # Forward propagation of the n_th sample\n",
    "        y_out = sigma(np.dot(W.T, x_n)).item()\n",
    "\n",
    "        # Let's keep track of the sum of squared errors\n",
    "        mse[it] +=  # TO COMPLETE compute squared error between y_target and y_out\n",
    "        # compute the gradient\n",
    "        grad = # TO COMPLETE\n",
    "\n",
    "        # Calculate the weights update\n",
    "        W_delta = - learning_rate * grad * x_n\n",
    "\n",
    "        # Update the weights\n",
    "        W += W_delta\n",
    "\n",
    "    # Divide by the number of elements to get the mean of the squared errors\n",
    "    mse[it] /= len(X)\n",
    "\n",
    "# Now let's see the output for each input sample with the trained weights\n",
    "# Using batch mode (see next section) we can do this in a single line\n",
    "print(\"Output after training, y_out =\")\n",
    "y_out = sigma(np.dot(X, W))\n",
    "print(y_out)\n",
    "print(\"Target output, y =\")\n",
    "print(y)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDCODE`"
   ],
   "metadata": {
    "id": "r4TFfm4tSmHl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q2: Fourth sample **[TO COMPLETE]**\n",
    "Why is the estimation for the fourth sample way different than its real label value? What adjustment can you consider to make this sample get classified correctly as well? (You do not have to implement the adjustment)"
   ],
   "metadata": {
    "id": "pF3Gu_OpRwJh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTEXT`"
   ],
   "metadata": {
    "id": "0aSKWdX2S3TF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Answer: **[TO COMPLETE]**"
   ],
   "metadata": {
    "id": "nua5TaoySA0m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDTEXT`"
   ],
   "metadata": {
    "id": "61rcR2r-TBiV"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT1qLBvBzpDc"
   },
   "source": [
    "After the training phase, the output of the network is fairly close to the target output.\n",
    "\n",
    "\n",
    "How many iterations were required in order to obtain this result? We have set the number of the iteration to $1000$, but it is interesting to investigate the trend of the error through the training. In the next homework, we will discuss how to select the right number of iterations (also known as *epochs*), for now let's just plot its behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a6RsKlA7zrw3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "outputId": "d22cce72-b877-4458-ebee-0e85b13112b7"
   },
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_iters), mse, label=\"MSE\")\n",
    "plt.xlabel(\"# Iterations\")\n",
    "plt.title(\"MSE behaviour\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiS6hr-eMWR1"
   },
   "source": [
    "You should see the error going down pretty quickly in the beginning and then slowing down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zoz4y02rxorc"
   },
   "source": [
    "### Batch training\n",
    "\n",
    "With real scenarios it is inefficient to handle each example individually. Our whole dataset can be forward propagated without a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pwc6hfIlxspG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "03c0ef68-afa8-47a4-a435-cbaaee78a9d4"
   },
   "source": [
    "y_out = sigma(np.dot(X, W))\n",
    "print(\"y_out =\", y_out)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8pBN8voyLGv"
   },
   "source": [
    "so we will get the corresponding output (each value in `y_out`) for each input (each row in `X`) in single matrix multiplication.  The error and weight updates can all be calculated in a single go, using matrix multiplications similarly to the steps we did above with single vectors.\n",
    "\n",
    "However, in these exercises you can (but you do not have to) stick to looping over one sample at a time, as we will deal with the batch training mode in the next homeworks and in this case it does not lead to any significant speed advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.2: 2D XOR Problem\n",
    "\n",
    "Now let's try a slightly more difficult example. It is a 2D variant of the XOR problem, in which we lay points in a 2D space ideally reproducing the output of the logical function XOR. Here's a visualization of the points in 2D:"
   ],
   "metadata": {
    "id": "t1NBjmZ5K_Nb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rng = np.random.default_rng(seed=[42,2])\n",
    "\n",
    "def flip(p=0.1):\n",
    "  if rng.random() < p:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def XOR_data_generator(n):\n",
    "  data = rng.random(size=(n, 2))\n",
    "  clusters = {(0.25, 0.25):0, (0.25, 0.75):1,\n",
    "              (0.75, 0.25):1, (0.75, 0.75):0}\n",
    "  labels = [0 for _ in range(n)]\n",
    "  for idx, p in enumerate(data):\n",
    "    max = 0\n",
    "    for c in clusters:\n",
    "      distance = np.sum(np.power((p - np.array(c)), 2))\n",
    "      if distance > max:\n",
    "        labels[idx] = clusters[c]\n",
    "        max = distance\n",
    "    if flip():\n",
    "      labels[idx] = np.abs(labels[idx])\n",
    "  return data, np.array(labels).reshape(-1, 1)\n",
    "\n",
    "X, y = XOR_data_generator(200)\n",
    "\n",
    "plt.title(\"Noisy XOR Dataset\")\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "W5cyx0wyLBpz",
    "outputId": "d0de42d7-1a49-4acc-e463-66d26bbe78b9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that the XOR function has the following truth table:\n",
    "\n",
    "||Input|    Output|\n",
    "|:----:|:---:|---:|\n",
    "| 0    | 0   | 1  |\n",
    "| 0    | 1   | 0  |\n",
    "| 1    | 0   | 0  |\n",
    "| 1    | 1   | 1  |\n",
    "\n",
    "You can see how points above and below `0.5` on each axis can be interpreted as \"true\" or \"false\" values, respectively.\n",
    "\n",
    "This problem is interesting because it can not be solved by using a single layer perceptron. Indeed, you will need (at least) a two-layer network to solve it (if this not obvious to you, revise the class material, think about it and come back later!).\n",
    "\n",
    "We will now visually represent the classification boundary that a perceptron learns when we try to fit it to these data points, and observe that‚Äîexpectedly‚Äîit does not separate the two classes.\n"
   ],
   "metadata": {
    "id": "7V-o8iGoG3-h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single-Layer MLP"
   ],
   "metadata": {
    "id": "oesDzp4tMHUK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rng = np.random.default_rng(seed=[42,3])\n",
    "\n",
    "# Weights initialization\n",
    "W = 2 * np.random.random((2,1)) - 1\n",
    "\n",
    "# Activation function\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x) )"
   ],
   "metadata": {
    "id": "Li0dgy7gLTrj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_iters = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for it in range(num_iters):\n",
    "    for n in range(len(X)):\n",
    "        x_n = np.reshape(X[n], (2,1))\n",
    "        y_target = y[n]\n",
    "\n",
    "        # Forward propagation\n",
    "        y_out = sigma(np.dot(W.T, x_n))\n",
    "        err = (y_out - y_target).item()\n",
    "\n",
    "        # Compute the gradient\n",
    "        grad = err*y_out*(1 - y_out)\n",
    "\n",
    "        # Calculate the weights update\n",
    "        W_delta = -learning_rate * grad * x_n\n",
    "\n",
    "        # Update the weights\n",
    "        W += W_delta\n",
    "\n",
    "    if it % 10 == 0: # Print every 10 epochs\n",
    "      print(f\"Iteration {it} -- Loss: {err:.6f}\")"
   ],
   "metadata": {
    "id": "y1SvguHsLTuR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "15463084-2088-4fcf-a0f4-ab52a7c956a9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def Classifier_plotter(X, y, m):\n",
    "  scale = np.max(X, axis=0)[0]\n",
    "  line = np.array([[0, 0], [scale, m*scale]])\n",
    "  plt.plot(line[:,0], line[:,1], label=\" MLP Classification Boundary\")\n",
    "\n",
    "  plt.scatter(X[:,0], X[:,1], c=y)\n",
    "  plt.xlim([-0.05,1.05])\n",
    "  plt.ylim([-0.05,1.05])\n",
    "  plt.legend()\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "zUnfXDXjLTxY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m = -W[0][0] / W[1][0]\n",
    "Classifier_plotter(X, y, m)"
   ],
   "metadata": {
    "id": "0oslY-4sLT0T",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "727766e0-2010-4dad-a1e3-444b252ce523"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q3: Two-Layer MLP [TO COMPLETE]\n",
    "We will now define a 2-layer feedforward network with a nonlinear activation, which will be able to represent the data points in a different 2D space in which they are linearly separable, and learn a classification boundary in that space.\n",
    "\n",
    "Once again, we will visually represent the data points in the new 2D space and the learned classification boundary. Notice that, to simplify the visualization of the datapoints in the new space, we will have just 2 nodes in the hidden layer."
   ],
   "metadata": {
    "id": "aOSGhQHGMLlw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_hidden = 2\n",
    "rng = np.random.default_rng(seed=[42,5])\n",
    "\n",
    "# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n",
    "W_1 = 2 * rng.random((2,num_hidden)) - 1\n",
    "W_2 = 2 * rng.random((num_hidden,1)) - 1"
   ],
   "metadata": {
    "id": "0ub7PCMHLT3J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTCODE`"
   ],
   "metadata": {
    "id": "4DZFyrqYXY38"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_iters = 2000\n",
    "learning_rate = 0.1 # learning rate\n",
    "\n",
    "mse = np.zeros(num_iters)\n",
    "\n",
    "for it in range(num_iters):\n",
    "    for n in range(len(X)):\n",
    "        x_n = np.reshape(X[n], (2,1))\n",
    "        y_target = y[n].item()\n",
    "\n",
    "        # Forward propagation\n",
    "        # Calculate h\n",
    "        h = sigma(np.dot(W_1.T, x_n))\n",
    "\n",
    "        # Calculate y_out\n",
    "        y_out = sigma(np.dot(W_2.T, h)).item()\n",
    "\n",
    "        # Let's keep track of the sum of squared errors\n",
    "        mse[it] += # TO COMPLETE\n",
    "\n",
    "        # Compute the gradient\n",
    "        grad =  # TO COMPLETE\n",
    "\n",
    "        W_2_delta = -learning_rate * grad * h\n",
    "\n",
    "        # Calculate the weight updates for W_1\n",
    "        # hint: you can either do this in parallel with vectorized operations,\n",
    "        # or by performing a for loop over i (hidden nodes)\n",
    "        # and k (input nodes) and calculate each W_1_ik update separately\n",
    "        # TO COMPLETE\n",
    "        W_1_delta = np.zeros((num_hidden, len(x_n))) # Optional to initialize the matrix\n",
    "        for i in range(num_hidden):\n",
    "            # ...\n",
    "\n",
    "        # Update the weights, note: it's important the W weights are updated at the end,\n",
    "        # the above calculation should be done with the old weights\n",
    "        W_1 # TO COMPLETE\n",
    "        W_2 # TO COMPLETE\n",
    "\n",
    "    # Divide by the number of elements to get the mean of the squared errors\n",
    "    mse[it] /= len(X)\n",
    "\n",
    "    if it % 10 == 0:\n",
    "      print(f\"Iteration {it} -- Loss: {mse[it]:.6f}\")"
   ],
   "metadata": {
    "id": "wgp9yxsQMO2Y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "827f211a-a29d-4ee9-e28d-e6d57c014e06"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDCODE`"
   ],
   "metadata": {
    "id": "RmcNgOdVXgE0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_new = sigma(np.dot(X, W_1))\n",
    "m = -W_2[0][0] / W_2[1][0]\n",
    "Classifier_plotter(X_new, y, m)"
   ],
   "metadata": {
    "id": "duVjjZaqMO4_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "88b27573-9520-4fdd-fea2-d7cedc2d7cb5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see how in the latent space the points are almost perfectly divided by the classification boundary learned by the network."
   ],
   "metadata": {
    "id": "KrWyaQqwLdxi"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAmf-t0PurMP"
   },
   "source": [
    "## Exercise 1.3: Handwritten digits classification\n",
    "In the next two exercises we will try to apply what we learned in the previous ones in two slightly more realistic scenarios.\n",
    "\n",
    "In particular, we now consider a simple digits classification problem. The model turns out to be similar to the perceptron implemented in Exercise 1.1, but here we will use softmax activation function and cross-entropy loss function. The idea is to create a model that has in input an image of a handwritten digit and that return a vector of 10 probabilities (one for each possible digit $0-9$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAYb_cd8vkG-"
   },
   "source": [
    "### Dataset\n",
    "The dataset that we will use in this exercise is included in [scikit-learn](https://scikit-learn.org/stable/), one of the major Machine Learning libraries. The dataset is called `load_digits` and contains several hundreds of samples. Each datapoint is made of the handwritten digit image (or rather its $8\\times8$ pixel representation), that will be the input of our model, and the target digit value.\n",
    "\n",
    "Let's start by plotting one of this handwritten digit:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MGrkAeWtufMk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "outputId": "40b776d3-7c4a-4366-ed40-ab326ccd96fe"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "def plot_digit(x, y):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  plt.imshow(x, cmap=plt.cm.gray_r,\n",
    "            interpolation='nearest')\n",
    "  plt.title(\"Image True Label: %d\" % y)\n",
    "  plt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "  plt.show()\n",
    "\n",
    "sample_index = 42\n",
    "plot_digit(digits.images[sample_index], digits.target[sample_index])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0OsnNJbv69J"
   },
   "source": [
    "It is better to check how an input in the dataset $\\mathbf{X}$ and its related target $\\mathbf{y}$ are represented in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ei9Uda4av5q2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f53de81d-6e06-4418-9ac8-5995b40d8492"
   },
   "source": [
    "data = np.asarray(digits.images[sample_index], dtype='float32')\n",
    "target = np.asarray(digits.target[sample_index], dtype='int32')\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize) # In this way we print the full array\n",
    "print(\"X:\", data)\n",
    "print(\"y:\", target)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# the dataset is the flattened version of all the images ( 8 x 8 = 64 values for 1797 images)\n",
    "print(digits.data.shape)"
   ],
   "metadata": {
    "id": "HfYJPxfQl7jL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3d77b865-746d-4725-9bcd-549cf44d3f3d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "So essentialy the dataset is a matrix with the (color) values for each pixel and for each image, whereas the target is the digit itself."
   ],
   "metadata": {
    "id": "T9yg07qT3-Kr"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hc97pziwLKa"
   },
   "source": [
    "#### One-hot encoding\n",
    "In order to have a representation of the target that will be similar to the output of the model (i.e. $\\hat{y}=0$ or $1$ for each of the 10 digits), we will use one-hot encoding. Basically, the one-hot encoding allow us to encode a categorical integer feature using a one-of-K scheme, where each class is translated to a specific index of an array."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eF6jEnOmwRUk"
   },
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, if there are 10 classes in total and a sample belongs to class number 3, we can translate the output to the following length-ten array of 0 and 1 (class 3 is in the fourth index because we start counting from zero!):"
   ],
   "metadata": {
    "id": "onD0qqMx6shM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r4CISmJ39lts",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "25bb7bed-6119-43df-8340-ce89ebf6e59f"
   },
   "source": [
    "one_hot(n_classes=10, y=3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkSruP-bwkki"
   },
   "source": [
    "As activation function we will use the Softmax function: this particular function is very useful when we have to deal with multiclassification tasks and one-hot target because it turns numbers, a.k.a. logits (pre-activations), into $m$ probabilities that sum to one. Basically, Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes $j$:\n",
    "$$\n",
    "softmax(\\mathbf{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^{m}{e^{x_i}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X))"
   ],
   "metadata": {
    "id": "jRpVpZDGU2Cj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP7xOmNqsE-R"
   },
   "source": [
    "### Loss Function: Cross Entropy ###\n",
    "Usually, a neural network classifier that use the softmax function in the final layer is trained using Cross-Entropy as loss function:\n",
    "$$H(Y,P)=-E_{y \\sim Y}[log \\;P(y)]$$\n",
    "where $Y$ and $P$ are the true and predicted labels distributions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UYnPJ1lWsFkP"
   },
   "source": [
    "EPSILON = 1e-8 # this is needed for numerical stability\n",
    "\n",
    "def cross_entropy(Y_true, Y_pred):\n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred) # make sure the dimensions are right\n",
    "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "    return -np.mean(loglikelihoods)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2jR9ijztQWP"
   },
   "source": [
    "### Weights Initailiazation\n",
    "\n",
    "Similarly to what we did in previous exercises, we have to initialize the weights but in this case we will consider the bias term as well. Therefore, we define the weights $\\mathbf{W}\\in\\mathbb{R}^{m \\times n}$ and the bias $\\mathbf{b}\\in\\mathbb{R}^m$, where $n$ is the input size and $m$ is the number of classes.\n",
    "Now we can define the output of our model as\n",
    "\n",
    "$$\\hat{\\mathbf{y}}=softmax(\\textbf{W} \\textbf{x}+\\mathbf{b})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FtgOtaVL-4xd"
   },
   "source": [
    "rng = np.random.default_rng(seed=[42,10])\n",
    "\n",
    "input_size = digits.data.shape[1]\n",
    "n_classes = len(np.unique(digits.target))\n",
    "\n",
    "W = rng.uniform(size=(input_size,n_classes), high=0.1, low=-0.1) # Another way to sample from uniform distributions\n",
    "b = rng.uniform(size=n_classes, high=0.1, low=-0.1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ab3_mFz-9kG"
   },
   "source": [
    "Let's consider a sample from the training set, and plot the current output of our model before training it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6VpIr1KY_CTd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "outputId": "1cb5ce15-756b-4d05-8c67-425effacc87a"
   },
   "source": [
    "def plot_predictions(y_out, true_label):\n",
    "  plt.bar(range(n_classes), y_out, label=\"Predictions\", color=\"red\")\n",
    "  plt.ylim(0, 1)\n",
    "  plt.xticks(range(n_classes))\n",
    "  plt.legend()\n",
    "  plt.ylabel(\"Probability\")\n",
    "  plt.xlabel(\"Digit Class\")\n",
    "  plt.title(\"Image True Label: %d\" % true_label)\n",
    "  plt.show()\n",
    "\n",
    "y_out = softmax(np.dot(digits.data[sample_index], W) + b)\n",
    "plot_predictions(y_out, digits.target[sample_index])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the most likely predictions for our _untrained_ model is just $5$ (or it could have been any other random guess) for the handwritten $1$."
   ],
   "metadata": {
    "id": "LCdvwX-VjyeO"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4DYTEEh_QDf"
   },
   "source": [
    "### Q4: Training procedure [TO COMPLETE]\n",
    "As in the previous exercise let's define a training procedure. Note that in this case, we have to compute the gradient according to the softmax function and the loss function that the training has to optimize.\n",
    "\n",
    "Hence, the gradient for the weights $\\textbf{W}$ is:\n",
    "\n",
    "$\\nabla_W=(\\mathbf{\\hat{y}}-\\mathbf{y}) \\cdot \\mathbf{x}$\n",
    "\n",
    "while for the bias is:\n",
    "\n",
    "$\\nabla_b=(\\mathbf{\\hat{y}}-\\mathbf{y})$\n",
    "\n",
    "During the training procedure let's also compute the accuracy of the predictions and the loss value at each iteration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTCODE`"
   ],
   "metadata": {
    "id": "QvqY4kDVx8Fq"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OcQfByoE_a6v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d81316e-036d-4908-bf9e-44bc34759d2b"
   },
   "source": [
    "num_iters = 50 # Feel free to change this\n",
    "learning_rate = 0.0005 # Feel free to change this\n",
    "\n",
    "for it in range(num_iters):\n",
    "    iteration_accuracy = []\n",
    "    iteration_loss = []\n",
    "    for i, (X, y) in enumerate(zip(digits.data, digits.target)):\n",
    "\n",
    "        # Complete the training loop\n",
    "        # TO COMPLETE\n",
    "        y_out = \n",
    "        pred_err = \n",
    "\n",
    "        # Compute the gradient and update parameters\n",
    "        # TO COMPLETE\n",
    "        grad_W = \n",
    "        grad_b = \n",
    "        W = \n",
    "        b = \n",
    "\n",
    "        iteration_accuracy.append(np.argmax(y_out) == y)\n",
    "        iteration_loss.append(cross_entropy(one_hot(n_classes,y), y_out))\n",
    "\n",
    "    print(f\"Iteration: {it} -- Accuracy: {np.mean(np.asarray(iteration_accuracy)):.2%} -- Loss: {np.mean(iteration_loss):.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDCODE`"
   ],
   "metadata": {
    "id": "mo3tIVERyAUv"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnJH8lWaxtc4"
   },
   "source": [
    "As you should see during the training the accuracy increases after each iteration, while the loss function value progressively declines. To succeed in the assignement, you should reach at least Accuracy$>99.00\\%$ and MSE $< 0.05$.\n",
    "\n",
    "Finally, let's check how the prediction capability of our model changes after the training:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jGrzV9w6_lwz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "outputId": "4aa6c5f2-5c7d-4238-f455-323103942bb0"
   },
   "source": [
    "y_out = softmax(np.dot(digits.data[sample_index], W) + b)\n",
    "plot_predictions(y_out, digits.target[sample_index])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's test our model on custom generated images whose size are $8 \\times 8$. As you can see, we can simply simulate the number drawing by considering some non-zero pixels among zero-valued pixels. For example, the following test case includes `number 2` inside, that can be seen from positions of non-zero pixel locations as well."
   ],
   "metadata": {
    "id": "eCED9tQGbgz_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_num = 2\n",
    "px = 10\n",
    "test_img = np.array([\n",
    "                    [0.,  0.,  px,  px, px,  px,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  px,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  px,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., px,  px,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., px,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  px, 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  px,  px, 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  px,  px, px,  px,  0.,  0.]\n",
    "])\n",
    "plot_digit(test_img, test_num)\n",
    "\n",
    "test_img = test_img.flatten() # flatten the array to length (64, )\n",
    "y_pred = softmax(np.dot(test_img, W) + b)\n",
    "plot_predictions(y_pred, test_num)"
   ],
   "metadata": {
    "id": "LaV-ENlYWQ-q",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "outputId": "932aa560-f3ab-473c-c10c-9dd367cb2aa2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should try different numbers by changing the values in \"test_img\" array. You might observe and review the effect of `px` on the final estimation. Are you able to draw a number that is still recognizable for a human but not for the model?"
   ],
   "metadata": {
    "id": "imQEdCGDoJmT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_num = # [TO COMPLETE] Number you choose to draw on the test_img grid\n",
    "px = 10\n",
    "test_img = np.array([\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n",
    "                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.]\n",
    "])\n",
    "plot_digit(test_img, test_num)\n",
    "\n",
    "test_img = test_img.flatten() # flatten the array to length (64, )\n",
    "y_pred = softmax(np.dot(test_img, W) + b)\n",
    "plot_predictions(y_pred, test_num)"
   ],
   "metadata": {
    "id": "NCtOx-yLn7E-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "outputId": "670c83fc-63c9-423a-8fc7-28d3802afcd9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.4: Regression\n",
    "\n",
    "In this last exercise, to showcase the versatility of Deep Learning, we are going to train a Multi-Layer Perceptron to estimate the date of first performance of Shakespearean plays based on statistics about the words used in them (fancy, isn't it?). The dataset was first presented in [this paper](https://www.semanticscholar.org/paper/Multiple-regression-techniques-for-modeling-dates-Moscato-Craig/0e32c43760b42d78758097b30c9666aaf4f779c8).\n",
    "\n",
    "We will quickly go through it and prepare the data for you, there is no need to grasp all the details here. Your task is only to implement a 2-layer MLP model and training loop similarly to what we have done so far."
   ],
   "metadata": {
    "id": "nQA1GTUlLJXZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Download the Dataset\n",
    "\n",
    "! wget https://archive.ics.uci.edu/static/public/747/181+early+modern+english+plays+transcriptions+of+early+editions+in+tei+encoding.zip\n",
    "! unzip -f /content/181+early+modern+english+plays+transcriptions+of+early+editions+in+tei+encoding.zip"
   ],
   "metadata": {
    "id": "XgdEZmRCdSqL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset with Pandas and shuffle its rows.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/181_plays_1585-610_t.csv\") # Adjust path if you are not using Colab\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df.head()"
   ],
   "metadata": {
    "id": "6iw6pRvJLLxk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extracting X and y from the Pandas DataFrame\n",
    "\n",
    "X = np.array(df.iloc[:, 1:])\n",
    "y = np.array(df[\"Date of first performance (best guess)\"]).reshape(-1,1)\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "id": "l2Wi3GOKNH9i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Making into training and test splits with 85%-15% proportions. We will see later on how to use the validation set too.\n",
    "\n",
    "tr, tst = 0.85, 0.15\n",
    "n = int(len(df) * tr)\n",
    "X_train, X_test = X[:n, :], X[n:, :]\n",
    "y_train, y_test = y[:n], y[n:]"
   ],
   "metadata": {
    "id": "rrhivnr0NIG_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "id": "KlFAXj4_NIKh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data pre-processing\n",
    "\n",
    "To obtain meaningful performances with a basic MLP, we have to translate our tabular data into numerical vectors. Try to understand what each functions does, as these pre-processing steps are common practices in Data Science."
   ],
   "metadata": {
    "id": "hhI6r43xRPCu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Rescale training data in the interval [0, 1] and get the min and\n",
    "max coefficients to rescale also validation and test data based on\n",
    "the statistics of the training set, without leaking information.\n",
    "\"\"\"\n",
    "def fit_normalize(array, coeff1=1.0, coeff2=1.0):\n",
    "  minimum, maximum = coeff1*np.min(array), coeff2*np.max(array)\n",
    "  if minimum == maximum == 0: return (array, (0, 0))\n",
    "  scaled_array = (array - minimum) / (maximum - minimum)\n",
    "  return (scaled_array, (minimum, maximum))\n",
    "\n",
    "\"\"\"\n",
    "Function to rescale test data based on the statistics of the training set.\n",
    "\"\"\"\n",
    "def apply_normalize(array, param):\n",
    "  if param[0] == param[1] == 0: return array\n",
    "  scaled_array = (array - param[0]) / (param[1] - param[0])\n",
    "  return scaled_array\n",
    "\n",
    "\"\"\"\n",
    "Reverse function, to go from values in [0, 1] back to years.\n",
    "\"\"\"\n",
    "def reverse(array, params):\n",
    "  return params[0] + array * (params[1] - params[0])"
   ],
   "metadata": {
    "id": "v4lxe5lnNIOK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train_scaled = np.zeros_like(X_train, dtype=float)\n",
    "X_train_params = []\n",
    "for idx, col in enumerate(X_train.T):\n",
    "  X_train_scaled[: , idx], param = fit_normalize(col)\n",
    "  X_train_params.append(param)"
   ],
   "metadata": {
    "id": "AaPMTSp4Ndyp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_test_scaled = np.zeros_like(X_test, dtype=float)\n",
    "for idx, col in enumerate(X_test.T):\n",
    "  X_test_scaled[: , idx] = apply_normalize(col, X_train_params[idx])"
   ],
   "metadata": {
    "id": "Os3Akx20NXuZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_train_scaled, y_train_params = fit_normalize(y_train)\n",
    "y_test_scaled = apply_normalize(y_test, y_train_params)"
   ],
   "metadata": {
    "id": "5iol5X96NXxR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We aslo apply [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis?) to lower the dimensionality well below $51256$ while retaining the core of the data structure."
   ],
   "metadata": {
    "id": "QrizqQNT83VI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 100\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)"
   ],
   "metadata": {
    "id": "eCr9ppuLNlry"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train_scaled.shape, X_test_scaled.shape, y_train_scaled.shape, y_test_scaled.shape"
   ],
   "metadata": {
    "id": "_yaYAb8fNluw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q5: Training Procedure [TO COMPLETE]\n",
    "\n",
    "You will now have to implement a 2-layers MLP and its training loop.\n",
    " **Your goal is to reach a MSE on the test set below $0.03$**. There are many choices to make and tests to run, so do not worry if you do not get immediately this score! Please use the variable names and basic structure we provide, for all the rest you are free to experiment and try different approaches. We will see during the course and in the next homeworks how we can implement better and faster models, leveraging Deep Learning libraries."
   ],
   "metadata": {
    "id": "xXZQCnW8vpXM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%STARTCODE`"
   ],
   "metadata": {
    "id": "_mSSu6Xkv64T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rng = np.random.default_rng([42, 100])\n",
    "\n",
    "num_hidden = # TO COMPLETE (the higher the better, but more computationally expensive and leads to overfitting)\n",
    "\n",
    "# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n",
    "W_1 =  # TO COMPLETE\n",
    "W_2 =  # TO COMPLETE\n",
    "\n",
    "# Let us define a helper lambda function to make predictions for validation and testing\n",
    "forward_propagation = lambda x,w1,w2 : sigma(np.dot(sigma(np.dot(x, w1)), w2))"
   ],
   "metadata": {
    "id": "xer_n9AwNuri"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = X_train_scaled\n",
    "y = y_train_scaled\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "num_iters =  # TO COMPLETE\n",
    "learning_rate = # TO COMPLETE \n",
    "mse = np.zeros(num_iters)\n",
    "for it in range(num_iters):\n",
    "    for n in range(len(X)):\n",
    "        x_n = np.reshape(X[n], (n_components,1))\n",
    "        y_target = y[n].item()\n",
    "        # TO COMPLETE\n",
    "        h =\n",
    "        y_out = \n",
    "        mse[it] +=\n",
    "        grad = \n",
    "        W_2_delta = \n",
    "        W_1_delta = \n",
    "        # Compute grad_W_1 and W_1_delta\n",
    "        # ...\n",
    "        # Update the weights\n",
    "        W_1 \n",
    "        W_2 \n",
    "\n",
    "\n",
    "    mse[it] /= len(X)\n",
    "\n",
    "    y_test_scaled_pred = forward_propagation(X_test_scaled, W_1, W_2)\n",
    "    test_mse = np.mean(np.square(y_test_scaled_pred - y_test_scaled))\n",
    "\n",
    "    train_losses.append(mse[it])\n",
    "    test_losses.append(test_mse)\n",
    "    if it % 2 == 0:\n",
    "      print(f\"Iteration {it} -- train_loss: {mse[it]:.4f} -- test_loss {test_mse:.4f}\")"
   ],
   "metadata": {
    "id": "i5b96Ls0Nuuc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Nx5-fJZaupvf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_test_scaled_pred = forward_propagation(X_test_scaled, W_1, W_2)\n",
    "y_test_pred = reverse(y_test_scaled_pred, y_train_params)\n",
    "\n",
    "# Let's check whether you did all good!\n",
    "success = test_mse < 0.03\n",
    "print(f\"Test MSE = {test_mse}, < 0.03? {success}\")\n",
    "\n",
    "# Finally, let us compare our predicted years with the ground truth\n",
    "report = pd.DataFrame(None)\n",
    "report[\"y_test\"] = y_test.reshape(-1, )\n",
    "report[\"y_test_pred\"] = y_test_pred\n",
    "report.head()"
   ],
   "metadata": {
    "id": "N1WacLjYOA6Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`%ENDCODE`"
   ],
   "metadata": {
    "id": "yUqqhnhlv-Tv"
   }
  }
 ]
}
